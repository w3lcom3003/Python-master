{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e9e9d7a-021e-47dd-a936-38609b24a71a",
      "metadata": {
        "id": "5e9e9d7a-021e-47dd-a936-38609b24a71a",
        "outputId": "414b07f1-8b32-49e7-9313-536418d3947f"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "# 보안을 위해 따로 만든 .env파일을 불러올 수 있게 해주는 라이브러리\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv()\n",
        "# \"OPENAI_API_KEY\"라는 이름의 환경 변수 값을 가져옴\n",
        "open_ai_key = os.getenv('OPENAI_API_KEY')\n",
        "# 웬만하면 하드코딩하지 않는 것이 좋음\n",
        "client = OpenAI(api_key= 'sk-proj-IijgVfhp9Yd2dAYhmtEpT3BlbkFJah84IhiGypsfWzyuAOpV')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9e39f8-c20f-403e-9b90-147a289c650d",
      "metadata": {
        "id": "1b9e39f8-c20f-403e-9b90-147a289c650d"
      },
      "source": [
        "## API Key 작동 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df45369-c596-427c-b691-aa47280d6a45",
      "metadata": {
        "id": "7df45369-c596-427c-b691-aa47280d6a45"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key='sk-proj-IijgVfhp9Yd2dAYhmtEpT3BlbkFJah84IhiGypsfWzyuAOpV')\n",
        "\n",
        "# 답변 받을 수 있는 함수 제작\n",
        "def get_response(prompt):\n",
        "    # completions endpoint를 사용함\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt},],\n",
        "    )\n",
        "    # response는 JSON형태로 반환됨\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0406a8fc-2df0-4a5e-a1e0-eb0bae0807ac",
      "metadata": {
        "id": "0406a8fc-2df0-4a5e-a1e0-eb0bae0807ac",
        "outputId": "55397d26-fe5a-4727-9296-8837e28ab735"
      },
      "outputs": [],
      "source": [
        "response = get_response(\"하 힘들다\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37500986-ea23-4469-bd5a-5e75dc1c57b0",
      "metadata": {
        "id": "37500986-ea23-4469-bd5a-5e75dc1c57b0",
        "outputId": "bd80df13-38ba-4305-a41b-2cf3a7b8aaed"
      },
      "outputs": [],
      "source": [
        "# LLM의 다양한 자연어 처리 작업을 수행할 수 있도록 도움\n",
        "from llama_index.llms.openai import OpenAI as OpenAI_llama\n",
        "# ChatMessage는 텍스트를 생성하는 기능을 제공\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "# 새로운 객체 생성\n",
        "# 클라이언트 초기화\n",
        "client_llama = OpenAI_llama(model='gpt-3.5-turbo')\n",
        "\n",
        "# 프롬프트를 이용하여 텍스트 생성\n",
        "# 대화 시스템에 더욱 가까움\n",
        "# 이전 대화를 기억하게 할 수 있음\n",
        "messages = [ChatMessage(role=\"user\", content=\"안녕! 넌 누구야?\")]\n",
        "response = client_llama.chat(messages)\n",
        "response.message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0679183-2d3a-4bcc-a5af-ce4651e554f8",
      "metadata": {
        "id": "c0679183-2d3a-4bcc-a5af-ce4651e554f8"
      },
      "source": [
        "## LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6373fc36-b2d6-44d4-a337-15f198f72018",
      "metadata": {
        "id": "6373fc36-b2d6-44d4-a337-15f198f72018"
      },
      "source": [
        "### raw LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27110fd9-8bb1-4926-95a3-6458f8ea1e35",
      "metadata": {
        "id": "27110fd9-8bb1-4926-95a3-6458f8ea1e35",
        "outputId": "c1834f29-5896-470a-f3ce-1705ccdf5b20"
      },
      "outputs": [],
      "source": [
        "query = \"OpenAI의 sora 모델에 대해서 설명해줘.\"\n",
        "\n",
        "# 질문에 대해 단순히 답변을 받음\n",
        "resp = get_response(query)\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda1216e-22ce-4339-92b3-d38525727189",
      "metadata": {
        "id": "cda1216e-22ce-4339-92b3-d38525727189"
      },
      "source": [
        "### LLM with context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2294c5af-2924-4a80-9b5d-61c4a5f84043",
      "metadata": {
        "id": "2294c5af-2924-4a80-9b5d-61c4a5f84043",
        "outputId": "3f1efdfa-0adf-45a7-f88f-837d5242b27d"
      },
      "outputs": [],
      "source": [
        "# context를 주고 그 context에서 정보를 찾도록 하는 코드\n",
        "# f string을 이용함\n",
        "prompt = f\"\"\"\n",
        "Utilizing the given context, please answer the user question.\n",
        "\n",
        "[context]\n",
        "Text-to-video\n",
        "Sora\n",
        "Main article: Sora (text-to-video model)\n",
        "Sora is a text-to-video model that can generate videos based on short descriptive prompts[193] as well as extend existing videos forwards or backwards in time.[194] It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\n",
        "\n",
        "The team that developed Sora named it after the Japanese word for sky to signify its \"limitless creative potential\".[193] The technology behind Sora is an adaptation of the technology behind the DALL·E 3 text-to-image model.[195] OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for the purpose, but did not reveal the number or the exact source of the videos.[193]\n",
        "\n",
        "OpenAI demonstrated a few Sora-created high-definition videos to the general public on February 15, 2024, stating that the technology was able to generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model and the model's capabilities.[195] It acknowledged some shortcomings of the system, including struggles in simulating complex physics.[196] Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and may not be representative of Sora's typical output.[195]\n",
        "\n",
        "\n",
        "[user question]\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "resp = get_response(prompt)\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9e7c80-ecea-4e6e-9f75-05f194322916",
      "metadata": {
        "id": "ad9e7c80-ecea-4e6e-9f75-05f194322916",
        "outputId": "f284d040-e1d6-436c-b2b8-b73527a35892"
      },
      "outputs": [],
      "source": [
        "query = \"OpenAI의 sora 모델에 대해서 설명해줘.\"\n",
        "prompt = f\"\"\"\n",
        "Utilizing the given context, please answer the user question.\n",
        "\n",
        "[context]\n",
        "OpenAI Global LLC then announced its intention to commercially license its technologies.[47] It planned to spend the $1 billion \"within five years, and possibly much faster.\"[48] Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.[49]\n",
        "\n",
        "\n",
        "[user question]\n",
        "{query}\n",
        "\"\"\"\n",
        "# 찾는 정보가 context에 없다면 모른다고 출력함\n",
        "resp = get_response(prompt)\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e193d59-3237-40d5-bb54-aeb4e62d6455",
      "metadata": {
        "id": "5e193d59-3237-40d5-bb54-aeb4e62d6455"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba8f2af-b1c9-4921-8343-74115d532d83",
      "metadata": {
        "id": "fba8f2af-b1c9-4921-8343-74115d532d83"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89a8c65-e56e-47cc-8d5c-a43990616738",
      "metadata": {
        "id": "c89a8c65-e56e-47cc-8d5c-a43990616738"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1183ea-45ac-40f0-aae0-91f7cba955f3",
      "metadata": {
        "id": "0b1183ea-45ac-40f0-aae0-91f7cba955f3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "738f4c38-2b99-45c2-b5dc-485404fdfc20",
      "metadata": {
        "id": "738f4c38-2b99-45c2-b5dc-485404fdfc20"
      },
      "source": [
        "### 데이터 마련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef6c208-118f-4d29-bcdf-0877dee2b813",
      "metadata": {
        "id": "5ef6c208-118f-4d29-bcdf-0877dee2b813"
      },
      "outputs": [],
      "source": [
        "# 웹에서 데이터를 가져올 수 있도록 해주는 라이브러리\n",
        "import requests\n",
        "\n",
        "# get 함수를 사용하여 위키피디아의 데이터를 가져옴\n",
        "response = requests.get(\n",
        "    \"https://en.wikipedia.org/w/api.php\",\n",
        "    params={\n",
        "        # 수행해야 할 작업\n",
        "        \"action\": \"query\",\n",
        "        # 형식은 JSON\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": \"OpenAI\",\n",
        "        # 요약\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "    },\n",
        ").json()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c20376-cb59-4008-8234-994f4e9dfb4c",
      "metadata": {
        "id": "a2c20376-cb59-4008-8234-994f4e9dfb4c",
        "outputId": "b90b5054-1c13-41bf-b08d-a70ae8c5d24b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "response\n",
        "response['query']['pages']['48795986']['extract'].split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c9f346-b9c0-40e2-82b4-c3728cdb7065",
      "metadata": {
        "id": "61c9f346-b9c0-40e2-82b4-c3728cdb7065",
        "outputId": "1c64ac18-60b1-441b-da5f-9db416f4425a"
      },
      "outputs": [],
      "source": [
        "# 위 코드와 동일함\n",
        "# iterator는 index와 달리 데이터 내부 구조의 제약이 없음\n",
        "page = next(iter(response[\"query\"][\"pages\"].values())) # {'pageid': ...} 형태의 딕셔너리 반환\n",
        "text = page[\"extract\"]\n",
        "text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5f1475-0c79-4a25-803f-0c3a89d0ed4a",
      "metadata": {
        "id": "3f5f1475-0c79-4a25-803f-0c3a89d0ed4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2f6666-1bdc-4b5c-af0f-a022b0df6390",
      "metadata": {
        "id": "1c2f6666-1bdc-4b5c-af0f-a022b0df6390"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d35aa4-123c-441f-99b1-78c9dfe11fd9",
      "metadata": {
        "id": "e9d35aa4-123c-441f-99b1-78c9dfe11fd9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b15827d3-177c-461c-82dc-7851f93f47da",
      "metadata": {
        "id": "b15827d3-177c-461c-82dc-7851f93f47da"
      },
      "source": [
        "### indexing\n",
        "### chunk의 개념 이해하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8dda47-8a8c-45c3-9bd1-d2dbb631fa1d",
      "metadata": {
        "id": "6c8dda47-8a8c-45c3-9bd1-d2dbb631fa1d"
      },
      "outputs": [],
      "source": [
        "# 대규모 데이터 집합을 처리하기 위해 chunk를 사용\n",
        "def get_chunks(text, chunk_size):\n",
        "    # 공백을 기준으로 text를 나누고 words리스트에 저장함\n",
        "    words = text.split()\n",
        "    # ' '는 단어 사이의 구분자 역할\n",
        "    # chunk_size 개수 만큼의 단어들이 한 문장이 되어 chunk에 저장된다.\n",
        "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c56ed28f-c4f0-43c5-ba56-13919222db88",
      "metadata": {
        "id": "c56ed28f-c4f0-43c5-ba56-13919222db88",
        "outputId": "c7e6c898-3861-4b44-b815-4ceb34bb8f7a"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Sora\n",
        "Main article: Sora (text-to-video model)\n",
        "Sora is a text-to-video model that can generate videos based on short descriptive prompts[193] as well as extend existing videos forwards or backwards in time.[194] It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\n",
        "\n",
        "The team that developed Sora named it after the Japanese word for sky to signify its \"limitless creative potential\".[193] The technology behind Sora is an adaptation of the technology behind the DALL·E 3 text-to-image model.[195] OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for the purpose, but did not reveal the number or the exact source of the videos.[193]\n",
        "\n",
        "OpenAI demonstrated a few Sora-created high-definition videos to the general public on February 15, 2024, stating that the technology was able to generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model and the model's capabilities.[195] It acknowledged some shortcomings of the system, including struggles in simulating complex physics.[196] Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and may not be representative of Sora's typical output.[195]\"\"\"\n",
        "\n",
        "chunks = get_chunks(text, 128)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999fffad-3c34-4b20-be4c-e7f508b41750",
      "metadata": {
        "id": "999fffad-3c34-4b20-be4c-e7f508b41750",
        "outputId": "7c7ea3cf-7bd4-4d2e-ba96-82ffc64f31d9"
      },
      "outputs": [],
      "source": [
        "# \"안녕\\n나는\"이라는 텍스트를 임베딩하여 벡터로 반환해줌\n",
        "client.embeddings.create(input = \"안녕\\n나는\", model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10205cbd-6bf4-4be2-8f96-72b806417d4a",
      "metadata": {
        "id": "10205cbd-6bf4-4be2-8f96-72b806417d4a"
      },
      "outputs": [],
      "source": [
        "# 임베딩 값을 반환하는 함수\n",
        "def get_embedding(text):\n",
        "   return client.embeddings.create(input = [text], model=\"text-embedding-3-small\").data[0].embedding # [0.02429252117872238, -0.05541412532329559]처럼 리스트로 표현됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd2cafa-c03d-4e1c-98fb-be791603835f",
      "metadata": {
        "id": "2bd2cafa-c03d-4e1c-98fb-be791603835f",
        "outputId": "d167a8c7-2066-49a0-ee82-eaab9003ce58"
      },
      "outputs": [],
      "source": [
        "# chunks의 각 문장들을 모두 임베딩시켜서 출력함\n",
        "embeddings = [get_embedding(c) for c in chunks]\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d78865-b229-48b9-9877-25d6b877e603",
      "metadata": {
        "id": "e2d78865-b229-48b9-9877-25d6b877e603"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b84b80-e735-408a-8ecf-10244052bc9c",
      "metadata": {
        "id": "13b84b80-e735-408a-8ecf-10244052bc9c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96420c52-cecb-4be8-9927-3bb4a48eab4a",
      "metadata": {
        "id": "96420c52-cecb-4be8-9927-3bb4a48eab4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a51296a1-5e98-4702-a59d-b001881583f0",
      "metadata": {
        "id": "a51296a1-5e98-4702-a59d-b001881583f0"
      },
      "source": [
        "### retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e13897-153e-4da4-bfe5-a171470f1e2f",
      "metadata": {
        "id": "e9e13897-153e-4da4-bfe5-a171470f1e2f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# query는 사용자가 입력하는 문장\n",
        "# top_k는 사용자가 입력한 문장과 유사한 문장 중, 출력할 문장의 개수\n",
        "# db_embedding은 db에 있는 문장들을 임베딩 해둔 리스트\n",
        "def retriever(query, top_k, db_embedding):\n",
        "    # 사용자가 입력한 query를 임베딩하여 q_emb 변수에 저장함\n",
        "    q_emb = get_embedding(query)\n",
        "\n",
        "    # q_emb를 리스트로 만들고 db_embedding과의 코사인 유사도를 구함\n",
        "    # 벡터의 차원만 같다면, 행렬의 크기는 달라도 괜찮다.\n",
        "    sim_score = cosine_similarity([q_emb], db_embedding)[0]\n",
        "    # 내림차순으로 정리하고, 유사도가 큰 문장을 top_k개 만큼 저장\n",
        "    # argsort 함수는 오름차순으로 '인덱스'를 반환함\n",
        "    max_indices = np.argsort(sim_score)[::-1][:top_k] # 인덱스 값\n",
        "\n",
        "    # i는 index 변수\n",
        "    retrieved_datas = [chunks[i] for i in max_indices]\n",
        "\n",
        "    return retrieved_datas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee993c38-07b8-4369-b509-815b2c54d0cd",
      "metadata": {
        "id": "ee993c38-07b8-4369-b509-815b2c54d0cd",
        "outputId": "08a9f6f7-8b98-45a1-fe3b-e27331c92c8a"
      },
      "outputs": [],
      "source": [
        "# embeddings는 위에서 만든 벡터 임베딩화한 데이터셋\n",
        "contexts = retriever(\"OpenAI의 sora 모델에 대해 설명해줘\", 3, embeddings) # LLM에 전달할 유사한 문장 3개를 출력해보는 코드\n",
        "contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf1b8eb-2044-4019-9dd7-779c529b29eb",
      "metadata": {
        "id": "9bf1b8eb-2044-4019-9dd7-779c529b29eb"
      },
      "source": [
        "### Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0175528a-d377-4de8-a9f4-4e46e390610c",
      "metadata": {
        "id": "0175528a-d377-4de8-a9f4-4e46e390610c"
      },
      "outputs": [],
      "source": [
        "# query는 사용자가 입력한 문장\n",
        "# contexts는 이미 가지고 있는 데이터셋을 뜻함(리스트)\n",
        "def generator(query, contexts):\n",
        "    # 줄 바꿈을 구분자로 정하고 contexts에 있는 요소들을 합침\n",
        "    # 아까 출력한 3개의 문장들을 하나의 context로 합치는 코드\n",
        "    context = \"\\n\".join(contexts)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Utilizing the given context, please answer the user question.\n",
        "\n",
        "    [context]\n",
        "    {context}\n",
        "\n",
        "    [user question]\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    return get_response(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af96d565-8088-4c0a-a9f2-54515e2dba70",
      "metadata": {
        "id": "af96d565-8088-4c0a-a9f2-54515e2dba70",
        "outputId": "d2d662e4-c3bf-4656-ad2e-2118356b3b45"
      },
      "outputs": [],
      "source": [
        "generator(query, contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25418d4f-1161-4ff7-8b7e-e17a2cab5bb8",
      "metadata": {
        "id": "25418d4f-1161-4ff7-8b7e-e17a2cab5bb8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e196c798-68a4-4013-893b-1862208cd448",
      "metadata": {
        "id": "e196c798-68a4-4013-893b-1862208cd448"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d489fe1-d04d-40e7-a261-4abad065875f",
      "metadata": {
        "id": "4d489fe1-d04d-40e7-a261-4abad065875f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967dd442-0ec5-437f-a86a-5a3cc88afa9b",
      "metadata": {
        "id": "967dd442-0ec5-437f-a86a-5a3cc88afa9b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fff8af-1f2b-4343-b70d-2573fccc2a48",
      "metadata": {
        "id": "e1fff8af-1f2b-4343-b70d-2573fccc2a48"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454d2bec-2500-485a-9fc6-09c3e7903b9e",
      "metadata": {
        "id": "454d2bec-2500-485a-9fc6-09c3e7903b9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0297b3c-9f87-4b49-9e67-cde1f862d765",
      "metadata": {
        "id": "e0297b3c-9f87-4b49-9e67-cde1f862d765"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f11cd119-ffb8-441a-896f-449790c30b6e",
      "metadata": {
        "id": "f11cd119-ffb8-441a-896f-449790c30b6e"
      },
      "source": [
        "# Llamaindex\n",
        "## loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536621dc-f46d-4fa1-b2f7-49cb774e5c98",
      "metadata": {
        "id": "536621dc-f46d-4fa1-b2f7-49cb774e5c98"
      },
      "outputs": [],
      "source": [
        "# 경로 조작 및 파일 시스템 작업을 직관적으로 처리\n",
        "from pathlib import Path\n",
        "\n",
        "# expanduser() 메서드는 경로에 ~(틸드)가 포함된 경우 이를 사용자 홈 디렉토리로 확장해 준다. 하지만 이 예제에서는 경로에 ~가 없으므로 별다른 변화는 없다.\n",
        "data_path = Path(r\"C:\\Users\\RSM\\OneDrive\\바탕 화면\\API 스터디\").expanduser()\n",
        "\n",
        "# openai.txt 에 넣고 싶은 텍스트\n",
        "text = \"w3lcom3003\"\n",
        "\n",
        "# 만약 data_path가 존재하지 않으면, 해당 디렉토리를 생성함\n",
        "if not data_path.exists():\n",
        "    Path.mkdir(data_path)\n",
        "\n",
        "# with는 작업이 끝나면 자동으로 파일을 닫아줌\n",
        "# openai.txt 파일을 쓰기 전용으로 연다.\n",
        "# 파일이 없으면 생성, 있다면 기존 내용을 덮어 쓴다.\n",
        "with open(f\"{data_path}/openai.txt\", \"w\") as fp:\n",
        "    fp.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1cd77a-5a20-4bc0-8f9c-c1ae73dcdad7",
      "metadata": {
        "id": "3e1cd77a-5a20-4bc0-8f9c-c1ae73dcdad7"
      },
      "outputs": [],
      "source": [
        "# VectorStoreIndex는 문서의 벡터 인덱스를 생성하고 관리함\n",
        "# SimpleDirectoryReader는 디렉토리나 파일에서 데이터를 읽어오는 클래스이다.\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "file_path = r\"C:\\Users\\RSM\\OneDrive\\바탕 화면\\API 스터디\"\n",
        "\n",
        "# load_data()는 데이터를 읽어와 문서 객체로 변환함\n",
        "documents = SimpleDirectoryReader(file_path).load_data()\n",
        "# chunk로 분할, 벡터로 변환, 인덱싱\n",
        "# from_documents의 인자에 원하는 텍스트 분할기를 줘서 chunk_size를 조절할 수 있음 \n",
        "vector_index = VectorStoreIndex.from_documents(documents) # chunk_size 1024 \n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3913f283-30cb-4737-ae7c-58085e3a4b38",
      "metadata": {
        "id": "3913f283-30cb-4737-ae7c-58085e3a4b38",
        "outputId": "cee08bfd-332f-42eb-f1e6-f86039b64c5b"
      },
      "outputs": [],
      "source": [
        "# 위에서 생성된 vector_index로부터 쿼리 엔진을 생성\n",
        "# query engine이란 query를 실행하고 결과를 반환해주는 소프트웨어 컴포넌트\n",
        "query_engine = vector_index.as_query_engine()\n",
        "# 질문에 대한 응답 받기\n",
        "response = query_engine.query(\"OpenAI의 sora 모델에 대해서 알려줘\")\n",
        "print(response)\n",
        "\n",
        "# 쿼리 엔진에서 생성된 프롬프트를 출력함\n",
        "# 프롬프트의 구조를 파악 가능\n",
        "prompt = query_engine.get_prompts()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a93a26-b61f-4ef6-bd82-2b5fb0f47126",
      "metadata": {
        "id": "88a93a26-b61f-4ef6-bd82-2b5fb0f47126",
        "outputId": "661d9d06-6562-4ba5-a11e-6f3b0cbcf9c8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a89caf-41e5-45f1-ba76-7ea5ef7665dc",
      "metadata": {
        "id": "09a89caf-41e5-45f1-ba76-7ea5ef7665dc",
        "outputId": "54ef4a59-8f45-4ecd-8852-93d9996d98cf"
      },
      "outputs": [],
      "source": [
        "Settings._llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab4b3e4-9b24-464d-bebd-124e6a967db9",
      "metadata": {
        "id": "5ab4b3e4-9b24-464d-bebd-124e6a967db9",
        "outputId": "4d1c0856-6d20-43c7-b3fc-449a6b9d4317"
      },
      "outputs": [],
      "source": [
        "Settings._embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9113bbaa-86a1-4a8d-9b0e-2c1a1029cbf3",
      "metadata": {
        "id": "9113bbaa-86a1-4a8d-9b0e-2c1a1029cbf3",
        "outputId": "be3bf0c7-d63e-43bb-ac26-e7d6087d38f7"
      },
      "outputs": [],
      "source": [
        "Settings._node_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a77a8c-4881-4df6-b5f3-611e9fbdc5a3",
      "metadata": {
        "id": "40a77a8c-4881-4df6-b5f3-611e9fbdc5a3"
      },
      "source": [
        "### Chunk size 조절하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff976ca-a444-4e16-9f18-0f6e783ccbba",
      "metadata": {
        "id": "aff976ca-a444-4e16-9f18-0f6e783ccbba",
        "outputId": "7b7c02db-a1fe-40ec-ec53-6a6126ca2eb0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 문장을 쪼개기 위해 SentenceSplitter 클래스 호출\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# chunk_overlap을 통해 일부러 단어들을 겹치게 하여 데이터 손실을 줄인다.\n",
        "text_splitter = SentenceSplitter(chunk_size=128,\n",
        "                                chunk_overlap = 10)\n",
        "\n",
        "# 문서를 청크로 나눈다. (노드)\n",
        "nodes = text_splitter.get_nodes_from_documents(documents)\n",
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8dcf2f1-7950-4bf9-b87e-2eea8406bc18",
      "metadata": {
        "id": "f8dcf2f1-7950-4bf9-b87e-2eea8406bc18",
        "outputId": "3a916499-fe4a-4a92-c284-0ec850e89df5"
      },
      "outputs": [],
      "source": [
        "nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fb8eea",
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf6319a-0d0a-4e6d-a3ff-9f693fe86124",
      "metadata": {
        "id": "bbf6319a-0d0a-4e6d-a3ff-9f693fe86124",
        "outputId": "99cb10f3-defe-4258-96f8-2ef2cb3f0b43",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "# 메타데이터 없이 출력\n",
        "text_splitter = TokenTextSplitter(chunk_size=128, # 기본값을 4000토큰\n",
        "                                chunk_overlap = 10, # 기본값은 200토큰\n",
        "                                include_metadata=False)\n",
        "\n",
        "# documents를 클래스 객체 안의 함수를 통해 쪼개서 노드를 생성\n",
        "nodes = text_splitter.get_nodes_from_documents(documents)\n",
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5df658-5b31-43b1-aa21-d6196fbdd7ac",
      "metadata": {
        "id": "4e5df658-5b31-43b1-aa21-d6196fbdd7ac",
        "outputId": "e24d831f-8ff2-4f2a-d9b7-277affcb4ddf"
      },
      "outputs": [],
      "source": [
        "nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f26fae",
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f875f4-c0aa-46ad-9010-f6ee0c11e0d8",
      "metadata": {
        "id": "c4f875f4-c0aa-46ad-9010-f6ee0c11e0d8"
      },
      "source": [
        "### Append node to vector index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59af0f26-ad90-4b17-897d-069c22cc282f",
      "metadata": {
        "id": "59af0f26-ad90-4b17-897d-069c22cc282f"
      },
      "outputs": [],
      "source": [
        "# TextNode 클래스는 텍스트 데이터를 구조화할 수 있도록 도와준다\n",
        "from llama_index.core.schema import TextNode\n",
        "\n",
        "# 텍스트에 대한 노드를 생성\n",
        "node_new = TextNode(text = \"OpenAI의 소라는 text to video 모델로, 생성할 수 있는 비디오는 오직 2d 애니메이션이다. 대표적인 창작물로는 NLP에 대한 소개 애니메이션이 있다.\")\n",
        "\n",
        "nodes = []\n",
        "\n",
        "nodes.append(node_new)\n",
        "\n",
        "print(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a1bbbb-f1df-4479-ac6b-f3091b3b5c52",
      "metadata": {
        "id": "c2a1bbbb-f1df-4479-ac6b-f3091b3b5c52",
        "outputId": "f36fe68f-e218-453b-96b9-1b031a07f366"
      },
      "outputs": [],
      "source": [
        "# nodes는 벡터 데이터의 모음\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "# 벡터 데이터에 대해 쿼리 엔진 생성\n",
        "query_engine = vector_index.as_query_engine()\n",
        "# 쿼리 실행\n",
        "response = query_engine.query(\"OpenAI의 sora 모델에 대해서 알려줘\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1965f59e-c3b2-4ad8-8e5e-5e875c5fc04c",
      "metadata": {
        "id": "1965f59e-c3b2-4ad8-8e5e-5e875c5fc04c"
      },
      "source": [
        "### Saving vector index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b67982-b9af-496e-ba46-f7e320014b59",
      "metadata": {
        "id": "e6b67982-b9af-496e-ba46-f7e320014b59"
      },
      "outputs": [],
      "source": [
        "# vector_index = VectorStoreIndex(nodes[:-1])\n",
        "\n",
        "# 현재 벡터 인덱스 상태를 파일 시스템에 저장한다.\n",
        "vector_index.storage_context.persist(r\"C:\\Users\\RSM\\OneDrive\\바탕 화면\\API 스터디\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8feaeb-23f9-40b0-b683-154e093471a5",
      "metadata": {
        "id": "5d8feaeb-23f9-40b0-b683-154e093471a5",
        "outputId": "bc427c60-0514-4d98-fa89-5dd40d4d5043"
      },
      "outputs": [],
      "source": [
        "response = get_response(\"Mistral AI는 어느 국적의 회사야?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c773c186-4792-43d9-8dfe-6ab409afdc5a",
      "metadata": {
        "id": "c773c186-4792-43d9-8dfe-6ab409afdc5a",
        "outputId": "7c6fa3d5-770b-4a1e-eb1d-13cd1bc5ebe5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# 주어진 경로의 파일 데이터를 불러옴\n",
        "documents = SimpleDirectoryReader(r\"C:\\Users\\RSM\\OneDrive\\바탕 화면\\API 스터디\").load_data()\n",
        "# 불러온 파일 데이터를 벡터 인덱스로 바꿈\n",
        "vector_index = VectorStoreIndex.from_documents(documents)\n",
        "# 쿼리 엔진 실행\n",
        "query_engine = vector_index.as_query_engine()\n",
        "response = query_engine.query(\"Mistral AI는 어느 국적의 회사야?\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d992214",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# persist인자를 통해 저장 디렉토리를 지정\n",
        "storage_context = StorageContext.from_defaults(persist_dir=r\"C:\\Users\\RSM\\OneDrive\\바탕 화면\\API 스터디\")\n",
        "# 저장된 벡터 인덱스를 읽어 들인다.\n",
        "vector_index = load_index_from_storage(storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e12a3a6-558b-4f78-9811-4400777685a1",
      "metadata": {
        "id": "5e12a3a6-558b-4f78-9811-4400777685a1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0effba-bbbb-4b35-9ba2-05cac8fcb158",
      "metadata": {
        "id": "ad0effba-bbbb-4b35-9ba2-05cac8fcb158"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc1941b-fe1f-455d-84c9-3e2be14f82ed",
      "metadata": {
        "id": "1bc1941b-fe1f-455d-84c9-3e2be14f82ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "82a10332-2413-495a-be7b-c998f5ab6d62",
      "metadata": {
        "id": "82a10332-2413-495a-be7b-c998f5ab6d62"
      },
      "source": [
        "## Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611d2868-baa0-4996-91b4-c072e5802701",
      "metadata": {
        "id": "611d2868-baa0-4996-91b4-c072e5802701",
        "outputId": "71ad1b84-2a69-4a4d-f539-ff75502f733a"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# 임베딩 모델과 벡터 차원을 지정\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=128)\n",
        "li = list(embed_model.get_text_embedding('머신러닝은 멋져'))\n",
        "len(li) # 128 차원의 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f62da5-9c3e-474b-b77c-8f3fc26427dc",
      "metadata": {
        "id": "f4f62da5-9c3e-474b-b77c-8f3fc26427dc"
      },
      "outputs": [],
      "source": [
        "Settings.embed_model = embed_model\n",
        "vector_index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db4e63b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Settings.embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "706fba99-8167-401f-98e7-266df721bd2d",
      "metadata": {
        "id": "706fba99-8167-401f-98e7-266df721bd2d"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    # 검색에 사용할 벡터 인덱스 지정\n",
        "    index=vector_index,\n",
        "    # 유사한 항목 수 지정\n",
        "    similarity_top_k=3,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9455a37d-bde2-4e44-a4c4-081c1d7bd912",
      "metadata": {
        "id": "9455a37d-bde2-4e44-a4c4-081c1d7bd912",
        "outputId": "b8d1ea8d-8e86-4d32-924a-0ba57383a0c4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "nodes = retriever.retrieve(\"Explain what OpenAI Sora model is.\")\n",
        "\n",
        "for node in nodes:\n",
        "    print(node.text)\n",
        "    print(\"===\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2547d431-530d-47fc-9205-75a7c0b09856",
      "metadata": {
        "id": "2547d431-530d-47fc-9205-75a7c0b09856"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "108d8d2a-f9ed-4e0e-b258-c25c1c2b43b5",
      "metadata": {
        "id": "108d8d2a-f9ed-4e0e-b258-c25c1c2b43b5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253b5b11-75aa-478d-ad6e-128c9313bda0",
      "metadata": {
        "id": "253b5b11-75aa-478d-ad6e-128c9313bda0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "db4243da-a531-4f8b-8bd1-6977d0fc7f73",
      "metadata": {
        "id": "db4243da-a531-4f8b-8bd1-6977d0fc7f73"
      },
      "source": [
        "### Doc summary index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891edc84-ae34-4e45-8c63-b89ecd95d7c8",
      "metadata": {
        "id": "891edc84-ae34-4e45-8c63-b89ecd95d7c8",
        "outputId": "06263f84-3abf-49c1-ea7d-a341246efe46"
      },
      "outputs": [],
      "source": [
        "# 문서 요약해주는 클래스\n",
        "from llama_index.core import DocumentSummaryIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# 모델 지정\n",
        "chatgpt = OpenAIEmbedding(model = \"text-embedding-3-small\", metadata = None)\n",
        "\n",
        "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
        "    documents,\n",
        "    llm=chatgpt,\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38c1e13-3e1a-43a6-9580-faf58a764857",
      "metadata": {
        "id": "a38c1e13-3e1a-43a6-9580-faf58a764857",
        "outputId": "01469ce5-5578-44ef-ca9b-a35db785fb26"
      },
      "outputs": [],
      "source": [
        "doc_summary_index.get_document_summary(documents[0].id_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c376eaa3-bb8e-492e-837e-149f6d1bbe35",
      "metadata": {
        "id": "c376eaa3-bb8e-492e-837e-149f6d1bbe35",
        "outputId": "7ffa45c4-374b-4838-d884-4fd12da970fa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.document_summary import (\n",
        "    DocumentSummaryIndexLLMRetriever,\n",
        ")\n",
        "retriever = DocumentSummaryIndexLLMRetriever(\n",
        "    index=doc_summary_index,\n",
        "    choice_batch_size=3,\n",
        "    similarity_top_k=3,\n",
        "\n",
        ")\n",
        "\n",
        "retriever.retrieve(\"Mistarl AI는 어느 국가에 속해 있어?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a4aa06-6497-4a29-9dbd-dd8ed1d880de",
      "metadata": {
        "id": "18a4aa06-6497-4a29-9dbd-dd8ed1d880de",
        "outputId": "a6622c4f-fdc3-4de2-fb29-27a587125ce4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.document_summary import (\n",
        "    DocumentSummaryIndexEmbeddingRetriever,\n",
        ")\n",
        "retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
        "    doc_summary_index,\n",
        "    similarity_top_k=3,\n",
        ")\n",
        "retriever.retrieve(\"Mistarl AI는 어느 국가에 속해 있어?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2827bc0-24be-41b9-96c8-92aa00a92021",
      "metadata": {
        "id": "e2827bc0-24be-41b9-96c8-92aa00a92021"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd313be1-4bbc-4a66-9649-d175a001e2f2",
      "metadata": {
        "id": "fd313be1-4bbc-4a66-9649-d175a001e2f2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145163e1-1081-45e3-a0c2-9e840d1898c4",
      "metadata": {
        "id": "145163e1-1081-45e3-a0c2-9e840d1898c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77299d3a-300a-4a02-86ba-8979e41418d6",
      "metadata": {
        "id": "77299d3a-300a-4a02-86ba-8979e41418d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8228cc7d-ea92-43bd-96ca-cf2b96620992",
      "metadata": {
        "id": "8228cc7d-ea92-43bd-96ca-cf2b96620992"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e2834495-3d95-4f53-ba6f-fd270f5fa32a",
      "metadata": {
        "id": "e2834495-3d95-4f53-ba6f-fd270f5fa32a"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ed6301-677d-4db3-ab89-08a6045aa8f0",
      "metadata": {
        "id": "73ed6301-677d-4db3-ab89-08a6045aa8f0",
        "outputId": "e30a26ef-4b86-4b97-a209-6f04634ee06a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "\n",
        "# 간결한 응답 요청\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
        "\n",
        "response = response_synthesizer.synthesize(\n",
        "    \"OpenAI의 Sora 모델에 대해서 설명해줘.\", nodes=retriever.retrieve(\"OpenAI의 Sora 모델에 대해서 설명해줘.\")\n",
        ")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26613f7a-33a0-4a50-88e8-c69936b4a167",
      "metadata": {
        "id": "26613f7a-33a0-4a50-88e8-c69936b4a167",
        "outputId": "f6dd7dc0-42a2-4e27-895a-2b831297ec38"
      },
      "outputs": [],
      "source": [
        "# 자세한 응답 요청\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"accumulate\")\n",
        "\n",
        "response = response_synthesizer.synthesize(\n",
        "    \"OpenAI의 Sora 모델에 대해서 설명해줘.\", nodes=retriever.retrieve(\"OpenAI의 Sora 모델에 대해서 설명해줘.\")\n",
        ")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad6a209-b0de-42a3-b950-9fd7a08955a8",
      "metadata": {
        "id": "fad6a209-b0de-42a3-b950-9fd7a08955a8",
        "outputId": "3419c1e7-3dcc-454b-f4f9-7035d63a5637"
      },
      "outputs": [],
      "source": [
        "# 간단한 요약 응답 요청\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"simple_summarize\")\n",
        "\n",
        "response = response_synthesizer.synthesize(\n",
        "    \"OpenAI의 Sora 모델에 대해서 설명해줘.\", nodes=retriever.retrieve(\"OpenAI의 Sora 모델에 대해서 설명해줘.\")\n",
        ")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7cc1e85-740c-41d9-8df0-d38c4a4703a3",
      "metadata": {
        "id": "f7cc1e85-740c-41d9-8df0-d38c4a4703a3",
        "outputId": "516483fe-cf92-489d-fb25-16bc47919aa4"
      },
      "outputs": [],
      "source": [
        "# 계층적 요약 응답 요청\n",
        "# 긴 텍스트를 처리하기 위해서\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
        "\n",
        "response = response_synthesizer.synthesize(\n",
        "    \"OpenAI의 Sora 모델에 대해서 설명해줘.\", nodes=retriever.retrieve(\"OpenAI의 Sora 모델에 대해서 설명해줘.\")\n",
        ")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a616a011-52ff-4062-a3dd-6898b9fa2fb3",
      "metadata": {
        "id": "a616a011-52ff-4062-a3dd-6898b9fa2fb3",
        "outputId": "53d7d556-3e6b-4e7f-caef-a7c9848c5d75"
      },
      "outputs": [],
      "source": [
        "response_synthesizer.get_prompts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3393c873-5fc9-43aa-90ec-957dc4955a11",
      "metadata": {
        "id": "3393c873-5fc9-43aa-90ec-957dc4955a11"
      },
      "source": [
        "### Manage prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34c4fe6-cc53-4033-9482-cd927ea5839a",
      "metadata": {
        "id": "f34c4fe6-cc53-4033-9482-cd927ea5839a",
        "outputId": "f04bf854-c13a-4ca3-b4f4-faaa8b474b45"
      },
      "outputs": [],
      "source": [
        "prompts_dict = query_engine.get_prompts()\n",
        "\n",
        "prompts_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d7e8a5-c7aa-40e3-a146-f2a6b864253e",
      "metadata": {
        "id": "a7d7e8a5-c7aa-40e3-a146-f2a6b864253e",
        "outputId": "d4af6c0a-0426-4841-d05d-f559ca188767"
      },
      "outputs": [],
      "source": [
        "for prompt in prompts_dict.values():\n",
        "    print(prompt.get_template())\n",
        "    print(\"***********\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f11ea2-2c7a-4e92-b931-6ba7956497c3",
      "metadata": {
        "id": "a4f11ea2-2c7a-4e92-b931-6ba7956497c3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "\n",
        "new_prompt_qa = \"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Use proper Korean, and answer in the style of kindergarten teacher, gentle and enthusiastic.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "new_prompt_refine = \"\"\"\n",
        "The original query is as follows: {query_str}\n",
        "We have provided an existing answer: {existing_answer}\n",
        "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
        "------------\n",
        "{context_msg}\n",
        "------------\n",
        "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
        "Use proper Korean, and answer in the style of kindergarten teacher, gentle and enthusiastic.\n",
        "Refined Answer:\n",
        "\"\"\"\n",
        "\n",
        "new_prompt_qa = PromptTemplate(new_prompt_qa)\n",
        "new_prompt_refine = PromptTemplate(new_prompt_refine)\n",
        "\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": new_prompt_qa, \"response_synthesizer:refine_template\": new_prompt_refine}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357f0232-bfde-43f2-89d7-fa345e54e554",
      "metadata": {
        "id": "357f0232-bfde-43f2-89d7-fa345e54e554",
        "outputId": "63fa4497-06fb-4b7f-ff70-fef6e6605009"
      },
      "outputs": [],
      "source": [
        "query_engine.get_prompts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6b6a5b-5cc1-459a-a0b0-9d348d945115",
      "metadata": {
        "id": "ed6b6a5b-5cc1-459a-a0b0-9d348d945115",
        "outputId": "47e58832-84a2-48c0-b46f-769e091cd897"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"MistralAI에 대해서 설명해줘\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d57847-a514-4614-ade6-e2a9903761b3",
      "metadata": {
        "id": "d3d57847-a514-4614-ade6-e2a9903761b3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
